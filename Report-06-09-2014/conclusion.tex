%In this work, we tuned six representative Polybench kernels for energy on
%an Intel SandyBridge processor and an Intel Xeon Phi coprocessor by applying various loop 
%transformations. For the \texttt{2mm} and \texttt{gemm} kernels (dense matrix kernels),
%we observed \emph{non-correlated} speedups and energy savings over the baseline version,
%i.e. \emph{fastest execution does not guarantee fewest energy consumption}. 
%We also showed that good loop transformations for one architecture do not carry over to
%other architectures.
Is the above assumption true? How does a machine learning function make sure that it does 
not give up on vectorization until everything is visible? 
Are there relationships between optimizations that can be applied across applications?
Can this be generalized, i.e. more apps and more optimizations?

