%In this work, we tuned six representative Polybench kernels for energy on
%an Intel SandyBridge processor and an Intel Xeon Phi coprocessor by applying various loop 
%transformations. For the \texttt{2mm} and \texttt{gemm} kernels (dense matrix kernels),
%we observed \emph{non-correlated} speedups and energy savings over the baseline version,
%i.e. \emph{fastest execution does not guarantee fewest energy consumption}. 
%We also showed that good loop transformations for one architecture do not carry over to
%other architectures.
\begin{itemize}
\item Is the above assumption true?
\item How does a machine learning function make sure that it does 
not give up on vectorization until everything is visable?
\item Are there relationships between optimizations that can be 
applied across applications?
\item Can this be generallized, i.e. more apps and more optimizations?
\end{itemize}

